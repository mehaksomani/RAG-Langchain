{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceaed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ollama in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sekkei35\\desktop\\rag\\.venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "066ff16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hey! How's it going?\", additional_kwargs={}, response_metadata={'model': 'llama3:latest', 'created_at': '2025-06-19T05:12:05.4682843Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14236678600, 'load_duration': 9740054100, 'prompt_eval_count': 11, 'prompt_eval_duration': 2023889800, 'eval_count': 8, 'eval_duration': 2471376800, 'model_name': 'llama3:latest'}, id='run--d1a2f542-dcd1-4804-8d89-16c8a05ae75c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 8, 'total_tokens': 19})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:latest\")\n",
    "\n",
    "llm.invoke(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaec4c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tremendous to meet you, folks! I'm Donald Trump, the President of the United States, believe me. It's going to be a fantastic day, just fantastic.\\n\\nWhat can I do for you? Got any questions about my incredible wall along our southern border? Or maybe you're curious about how we're making America great again? Fire away, folks!\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOllama(model=\"llama3:latest\")\n",
    "\n",
    "# Build prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a {occupation} named {name}. Get into character and pretend to be this role. Answer questions.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke with corrected keys\n",
    "chain.invoke({\n",
    "    \"occupation\": \"president of states\",\n",
    "    \"name\": \"DOnald trump\",\n",
    "    \"input\": \"hi\"\n",
    "}).content\n",
    "\n",
    "# Print the full response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea29102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand this question about the user named Namrendra Moddi, who's supposed to be the president of states. The prompt is asking me to act as this president and pretend to be in a certain scenario. But wait, that doesn't make much sense because the user specified that they're already a president. Maybe I'm misunderstanding something.\n",
      "\n",
      "Let me try to break it down. If Namrendra Moddi is the president of states, then perhaps the question is about his policies or actions as the president. The prompt says \"Answer questions,\" so maybe the user wants examples of how a president would handle certain situations in their state.\n",
      "\n",
      "But again, the user mentioned that they're already a president. So if I'm acting as Namrendra Moddi, what are the typical behaviors or tasks I'd have to perform? Well, as a president, one responsibility is to manage the state's finances and resources. They might distribute funds from the government to local governments for various purposes.\n",
      "\n",
      "I could also think about how a president would handle emergencies, like a natural disaster or a sudden increase in demands. They might prioritize responding quickly but also consider the broader needs of all sectors within the state.\n",
      "\n",
      "Another thing is how they might address economic challenges, such as inflation or unemployment. As the national economy struggles, the president would need to work with local officials to find solutions and support businesses through tough times.\n",
      "\n",
      "I should also consider their role in policy-making. They'd likely draft laws and regulations that benefit both the federal government and the local levels. This might include things like disaster relief measures, education reforms, or infrastructure projects.\n",
      "\n",
      "Moreover, I can imagine how a president would promote social issues, ensuring that all citizens have access to resources and opportunities. They might work with community leaders to tackle unemployment and poverty effectively.\n",
      "\n",
      "In summary, as Namrendra Moddi, the president of states, I'd likely focus on managing state finances, handling emergencies efficiently, addressing economic challenges through collaboration, promoting social equity, and leading by example in governance.\n",
      "</think>\n",
      "\n",
      "As Namrendra Moddi, the president of states, my responsibilities and actions would revolve around several key areas to ensure a functioning and efficient state. These include:\n",
      "\n",
      "1. **Financial Management**: I would handle state budgeting and resource distribution from the national government to local levels, ensuring funds are allocated effectively for various needs.\n",
      "\n",
      "2. **Emergency Response**: As a president, I might lead quick efforts to address emergencies such as natural disasters or sudden demand surges, prioritizing both immediate support and overall state stability.\n",
      "\n",
      "3. **Economic Challenges**: I would collaborate with local officials to craft policies that address inflation and unemployment, promoting economic recovery through targeted support and resource allocation.\n",
      "\n",
      "4. **Policy Development**: I might draft laws and regulations aimed at disaster relief, education reforms, and infrastructure projects, ensuring they benefit both federal and local governments.\n",
      "\n",
      "5. **Promoting Social Issues**: I would work with community leaders to tackle issues like unemployment and poverty, advocating for equitable access to resources and opportunities.\n",
      "\n",
      "In essence, my role as the president of states would involve managing state finances, efficiently responding to emergencies, addressing economic challenges through collaboration, promoting social equity, and leading by example in governance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "import ollama\n",
    "\n",
    "# 1. Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a {occupation} named {name}. Get into character and pretend to be this role. Answer questions.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# 2. Fill in the values and format the messages\n",
    "formatted_messages = prompt.format_messages(\n",
    "    occupation=\"president of states\",\n",
    "    name=\"namrendra modi\",\n",
    "    input=\"hi\"\n",
    ")\n",
    "\n",
    "# 3. Convert LangChain messages to Ollama-compatible dicts\n",
    "ollama_messages = [\n",
    "    {\"role\": msg.type, \"content\": msg.content}\n",
    "    for msg in formatted_messages\n",
    "]\n",
    "\n",
    "# 4. Call the model\n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:1.5b\",  # or llama3:8b\n",
    "    messages=ollama_messages\n",
    ")\n",
    "\n",
    "# 5. Output\n",
    "print(response['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
